{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "[0.8  0.1  0.03 0.05 0.02]\n",
      "[[0.8 ]\n",
      " [0.1 ]\n",
      " [0.03]\n",
      " [0.05]\n",
      " [0.02]]\n",
      "(5, 4)\n",
      "[[0.8  0.8  0.8  0.8 ]\n",
      " [0.1  0.1  0.1  0.1 ]\n",
      " [0.03 0.03 0.03 0.03]\n",
      " [0.05 0.05 0.05 0.05]\n",
      " [0.02 0.02 0.02 0.02]]\n",
      "(5, 4)\n",
      "(4,)\n",
      "[ 0.26671291 -1.7432512   1.12530281 -0.13507866]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "print(hs.shape)\n",
    "a  = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "print(a)\n",
    "ar = a.reshape(T, 1)\n",
    "print(ar)\n",
    "ar = ar.repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "print(ar)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4)\n",
      "(2, 3, 4)\n",
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N, T, H = 2, 3, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a  = np.random.randn(N, T)\n",
    "\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n",
      "[[12 15 18 21]\n",
      " [48 51 54 57]]\n"
     ]
    }
   ],
   "source": [
    "t = np.arange(24).reshape(2, 3, 4)\n",
    "print(t)\n",
    "c = np.sum(t, axis=1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "-4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hs = np.array([[1,1,1,1],\n",
    "               [1,1,-1,-1],\n",
    "               [-1,-1,1,1],\n",
    "               [-1,-1,-1,-1]])\n",
    "\n",
    "h = np.array([1,1,-1,-1])\n",
    "\n",
    "for  h_temp in hs:\n",
    "    print(np.dot(h_temp, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "(10, 5, 4)\n",
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "[[ 2.13895413  0.16180462  2.05041054 -0.46732199  1.58900983]\n",
      " [ 0.65173633 -1.73086554  2.05438564  0.81005208 -1.53265012]\n",
      " [-1.32132097  2.20900552 -1.39824675 -2.65334664 -0.30308086]\n",
      " [ 0.60162264 -0.17431357 -0.65476625  1.57101249 -0.81945623]\n",
      " [-0.03013397 -0.19250297  0.0483796  -0.016538   -0.81958487]\n",
      " [ 1.62560246  0.09045867  0.39314173  1.68972043 -0.34632695]\n",
      " [-0.87209962  0.12861409 -0.0237351   1.11858666  0.355926  ]\n",
      " [-0.62680912 -3.45963901 -2.00985498 -0.19035552  4.36741573]\n",
      " [-1.6620005  -0.70177452 -1.55369295  1.89976814  1.24906882]\n",
      " [-0.74218738 -0.45686661 -0.1957335   0.17768398  1.81482014]]\n",
      "(10, 5)\n",
      "[[3.69751732e-01 5.11970688e-02 3.38420160e-01 2.72909642e-02\n",
      "  2.13340075e-01]\n",
      " [1.55222846e-01 1.43286343e-02 6.31129530e-01 1.81849160e-01\n",
      "  1.74698293e-02]\n",
      " [2.55798113e-02 8.73169689e-01 2.36858456e-02 6.75158807e-03\n",
      "  7.08130659e-02]\n",
      " [2.16321845e-01 9.95670328e-02 6.15825427e-02 5.70296939e-01\n",
      "  5.22316407e-02]\n",
      " [2.27293991e-01 1.93228860e-01 2.45858918e-01 2.30405377e-01\n",
      "  1.03212854e-01]\n",
      " [3.68676487e-01 7.94220267e-02 1.07496555e-01 3.93089571e-01\n",
      "  5.13153599e-02]\n",
      " [5.95554180e-02 1.62003994e-01 1.39110948e-01 4.35978774e-01\n",
      "  2.03350866e-01]\n",
      " [6.64825585e-03 3.91223858e-04 1.66747195e-03 1.02862509e-02\n",
      "  9.81006797e-01]\n",
      " [1.71442605e-02 4.47857239e-02 1.91053997e-02 6.03914172e-01\n",
      "  3.15050444e-01]\n",
      " [5.13783824e-02 6.83429405e-02 8.87365068e-02 1.28906698e-01\n",
      "  6.62635473e-01]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h  = np.random.randn(N, H)\n",
    "print(h.shape)\n",
    "\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "print(hr.shape)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "print(s)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da\n",
    "\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh\n",
    "\n",
    "\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
